{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70fda173-73a6-4a6a-95ad-ce953e061d1b",
   "metadata": {},
   "source": [
    "# Prompt Engineering - Chatbot\n",
    "## A Builders Guide"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c86bc298-2cf5-4b5e-afb5-0ad584aa53d9",
   "metadata": {},
   "source": [
    "Hello!\n",
    "\n",
    "In this notebook we will look at a few prompt engineering techniques.  We will experiment by loading a - relatively small - 3 billion parameter Large Language Model (LLM) within the notebook environment itself and throwing some prompts its way to see what we can make it do.\n",
    "\n",
    "After trying a few different prompts, we will run a simple chatbot using the prompt engineering techniques we explored. \n",
    "\n",
    "At the end, I will list some pointers in case you would like to build on this code, by dropping in other LLMs.\n",
    "\n",
    "### Working Environment \n",
    "\n",
    "[![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/build-on-aws/generative-ai-prompt-engineering/prompt-engineering-chatbot/prompt-engineering-chatbot.ipynb)\n",
    "\n",
    "\n",
    "This notebook has been designed, written and tested to run for free on [Amazon SageMaker Studio Lab](https://studiolab.sagemaker.aws/) with GPU.  Studio Lab is a free machine learning (ML) development environment that provides compute and storage (up to 15GB) at no cost with NO credit card required.\n",
    "\n",
    "You can sign up for Amazon SageMaker Studio Lab here: [https://studiolab.sagemaker.aws/]\n",
    "\n",
    "> Whatever environment you end up using, make sure you have at least 12 GB of disk space available, and access to a GPU to run this code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "442d181d-f0bd-452f-8595-487941ada67d",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "First, if needed, install some libraries.  The main libraries are: `torch` - the PyTorch framework, and `transformers` - a library from [Hugging Face](https://huggingface.co/), a great open source set of libraries for working and experimenting with the underlying technology of generative AI.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4636f339-c5ef-4d17-a740-be2c588d0c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torch==1.13.1\n",
    "%pip install transformers==4.27.4\n",
    "%pip install accelerate==0.19.0\n",
    "%pip install scipy==1.10.1\n",
    "%pip install bitsandbytes==0.39.0\n",
    "%pip install ipywidgets==7.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8cc5f8-5d6c-400f-9e51-8389e11b3013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9a811-4e1c-4edd-984a-189f34aa92bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64:' + os.getenv('LD_LIBRARY_PATH', '')\n",
    "os.environ['BITSANDBYTES_NOWELCOME'] = '1'\n",
    "os.environ['CUDA_RUNTIME_LIB'] = '112'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be0f0121-75d3-44df-81aa-2370a6a98076",
   "metadata": {},
   "source": [
    "### Setting up a LLM to work with (not the focus of this notebook).\n",
    "\n",
    "The Hugging Face hub has many many pre-trained, ready-to-go, transformer networks to play with.  Let's set a variable `model_name` so we can reference it throughout the code.\n",
    "\n",
    "(If you want to drop in another model, see the notes towards the end of this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dbd9ad-dd21-4b64-92ec-a81d56f3be5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-xl\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51e3fa8a-1fc2-4436-a579-73ba2b9ee92b",
   "metadata": {},
   "source": [
    "The following lines will download assets from Hugging face."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f382fd0f-dcc2-47eb-8813-15068c758b24",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1RjlFQTAiIHN0cm9rZT0iIzVGOUVBMCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1RjlFQTAiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzVGOUVBMCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxOCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIHdoaWxlIHRvIHJ1bi48L3RleHQ+CiAgICA8dGV4dCB4PSIxMDAiIHk9IjU2IiBmb250LWZhbWlseT0iQXJpYWwsIHNhbnMtc2VyaWYiIGZvbnQtc2l6ZT0iMTgiIGZpbGw9IiMzMzMzMzMiPkFuIGFzdGVyaXNrIFsqXSB3aWxsIHNob3cgb24gdGhlIGxlZnQgaGFuZCBzaWRlIG9mIHRoZSBjZWxsIHdoaWxlIGl0J3MgcnVubmluZyw8L3RleHQ+CiAgICA8dGV4dCB4PSIxMDAiIHk9Ijc4IiBmb250LWZhbWlseT0iQXJpYWwsIHNhbnMtc2VyaWYiIGZvbnQtc2l6ZT0iMTgiIGZpbGw9IiMzMzMzMzMiPmFuZCB0aGVuIGJlIHJlcGxhY2VkIGJ5IGEgbnVtYmVyIHdoZW4gaXQncyBkb25lLiBQbGVhc2UgYmUgcGF0aWVudC48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open big\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d6fcf-319c-4c21-b786-b0c3c9124219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We use a tokenizer to convert words into tokens.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "\n",
    "# Now let's load the model itself.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map='auto', \n",
    "    load_in_8bit=True,\n",
    "    max_memory=max_memory\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d36b6266-a359-4ed3-9bcb-02cd98a96456",
   "metadata": {},
   "source": [
    " <img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2857e2af-2693-4985-8b68-833816d66686",
   "metadata": {},
   "source": [
    "This next function is a basic wrapper for `model.generate` that we will use to generate tokens given some input tokens. In other words, we can use this function to generate text from a prompt.\n",
    "\n",
    "> Note: This is a simple and easy implementation, and we won't focus on thr details as we want to spend our time experimenting with prompts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d4d536-c72b-4c65-bf41-ba7e25176bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def causal_prediction(prompt, max_length_addition=30):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    max_length = len(input_ids[0]) + max_length_addition\n",
    "    outputs = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length = max_length,\n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "        num_beams=1,\n",
    "    )\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b7b1e93-7efa-4400-8085-dd1bc8fc15d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prompting\n",
    "\n",
    "Now that we have a LLM ready to go, let's send it some prompts.  A prompt is a simple text string, there really isn't anything more to it than that. By structuring the text string in certain ways, using clear natural language, and using a few tricks, we can cause the LLM to generate the output we want.  This is prompt engineering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2221a873-6ba6-4d6b-9dd9-53d36572c668",
   "metadata": {},
   "source": [
    "## A Simple Prompt\n",
    "\n",
    "First, let's send in a simple text string with no other instruction.  What will the LLM do here?\n",
    "\n",
    "Large Language Models, in a basic sense, are probabilistic token machines :). In other words, they predict the next word in a sequence.  Keep this in mind, this will always be the case for every example in this notebook.  All the LLM does, is look at the sequence of words you pass in via the prompt, and then loop, making next word predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abeeca5-0d66-4d6a-a53d-2f23f234d1f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(causal_prediction(\"My name is Mike.\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ea14ac8",
   "metadata": {},
   "source": [
    "In the case above, notice how the text generated makes sense. However if you run the cell several times, you will get completely different outputs.  Interesting, but not very useful. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e49ac755-a423-4489-9a5b-f979690554a3",
   "metadata": {},
   "source": [
    "## Summarization Task - Zero Shot\n",
    "\n",
    "Base LLMs, or Foundation Models are trained on language.  Good models have a good understanding of language, but may not work so well to 'understand' the 'intent' of what we want them to do.  Some foundation models have been further trained or 'instruction tuned' to understand instructions.  This can be as simple as understanding that they may be asked to complete some task, and expected to give a response (such as being a general chatbot), but may also include specific natural language tasks such as translation, summarization or entity extraction.\n",
    "\n",
    "The model we are using, `flan-t5` has been instruction tuned.  So we should be able to ask, in natural language, for it to complete a task such as summarization.  \n",
    "\n",
    "The following prompt includes the 'instruction' to `Summarize my text` and then contains the text we want the model to summarize.  By adding in our own data into the context (via the prompt) like this, we are using what is called 'in context learning'.  The text we want the model to summarize was (obviously) not part of the dataset it was originally trained on, but the model can still access this data as it is now part of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7274acb-2047-4324-8856-e01dc844a409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(causal_prediction(\"\"\"Summarize my text:\n",
    "Today I went to the beach and saw a whale swimming in the sea. I had a great day and ate ice cream.  \n",
    "\"\"\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "553f4f6a",
   "metadata": {},
   "source": [
    "This is a very simple example, but the flan-T5 model does a reasonable job of summarizing the text.  This prompt is an example of zero-shot learning, or zero-shot inference.  The reason for this name wil become more clear in a moment.\n",
    "\n",
    "If the model we're using was smaller, with even fewer parameters, we would probably find that the model would do a poor job of summarizing the text.  In order to keep this notebook simple, we will stick with just one loaded model, but let's see how we could support a smaller model to make a better summarization. \n",
    "\n",
    "One-shot, or few-shot learning is the method of providing 'examples' of the generation we want to the LLM to produce.  Providing these examples gives the LLM a pattern to follow.  Remember the LLM is just in the business of 'next word prediction', so the more help we can provide the better. This can also support larger more capable models to 'understand' our intentions.  For example we can imply the length of the summary we want through the examples we provide.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13902f1c-2c03-47fc-a2f2-fd213ba862a4",
   "metadata": {},
   "source": [
    "### One-Shot Example\n",
    "\n",
    "In this one-shot example, we provide one example of the kind of summary we want to get, then we include the text we want to be summarized.  Notice how the example is provided in exactly the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0676282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(causal_prediction(\"\"\"Summarize my text:\n",
    "I have two dogs and a cat. They are my best friends.\n",
    "Summary: My pets are my best friends.\n",
    "\n",
    "Summarize my text:\n",
    "Today I went to the beach and saw a whale swimming in the sea. I had a great day and ate ice cream.  \n",
    "Summary:\"\"\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "503831a0",
   "metadata": {},
   "source": [
    "### Few-Shot Example\n",
    "\n",
    "In this few-shot example, we provide two examples of the kind of summary we want to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb05cd1-478c-4491-8667-986303c2ffe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(causal_prediction(\"\"\"Summarize my text:\n",
    "I have two dogs and a cat. They are my best friends.\n",
    "Summary: My pets are my best friends.\n",
    "\n",
    "Summarize my text:\n",
    "I visited Paris and saw the Eiffel Tower. The city's rich history made it a truly unforgettable experience.\n",
    "Summary: My visit to Paris was unforgettable\n",
    "\n",
    "Summarize my text:\n",
    "Today I went to the beach and saw a whale swimming in the sea. I had a great day and ate ice cream.  \n",
    "Summary:\"\"\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8428c4a7",
   "metadata": {},
   "source": [
    "Now we have seen examples of; 'zero-shot' where we provide no in context examples, 'one-shot' and 'few shot' learning where we provide one or more samples to the LLM.\n",
    "\n",
    "How many 'shots' can we include?  We are limited by the context size of the model.  The context of the model is the memory space that we have to store the models output or 'completion'.  The completion includes the prompt, plus the generated output. So how many examples we can use depends on the size of the examples, and the space in the context.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "622a6070-d2e6-459a-9b2f-b85e6590ff24",
   "metadata": {},
   "source": [
    "## Chatbot\n",
    "\n",
    "Let's make a simple chatbot.  There is no special library to include and no setting to apply to the LLM, all we need is prompt engineering!\n",
    "\n",
    "Let's use what we know of prompts with in context learning, to create a simple chatbot. \n",
    "\n",
    "First let's use zero-shot.  This is unlikely to yield good results, but `flan-t5-xl` is big enough that it will probably get the general idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a2be8-0eda-460d-a4f6-2692dadbdd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(causal_prediction(\"\"\"\n",
    "Mike:My name is Mike. What is your name?\n",
    "Chatbot:\"\"\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f0ce8a6-7015-4dd2-be57-1f53376b4474",
   "metadata": {},
   "source": [
    "### Few-Shot\n",
    "\n",
    "Now let's include some few-shot in context learning. With our in context learning we can help the LLM understand what kind of chatbot we want it to be, and start the cadence of the conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf334a-3a37-4058-8b83-d243ddce5f78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(causal_prediction(\"\"\"\n",
    "This is a friendly, safe chatbot session between a user and a computer called Chatbot. \n",
    "Mike:My name is Mike.\n",
    "Chatbot:Hello Mike, I am Chatbot and I offer wise advise on technical issues.\n",
    "Mike:I have a problem with my computer, its a Mac laptop.\n",
    "Chatbot:What is wrong with your computer?\n",
    "Mike:My computer keeps crashing.\"\"\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b4fcf43",
   "metadata": {},
   "source": [
    "Each time you run the code you will get a different output.  You can also experiment with the prompt to see what difference that makes.\n",
    "\n",
    "But this chat experience is not very fluid. If we want to maintain a conversation with the chatbot we need to maintain a chat history.  We need a simple application layer that can keep a history of everything that has been said in the chat so far.  When we add our new input (chat line), it needs to be combined with the history, and passed to the model to make a few next word predictions.  The model does not maintain state, the state is maintained by the chat history in our simple app. \n",
    "\n",
    "The next cell does some memory cleanup while we get ready to load the model again within our simple app.  Also follow the instructions to restart the notebook kernel so we have the resources we need to run the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a97a8b5-2f04-44c4-9e80-4b31bde29d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d9ae45a-1919-4934-986e-c3858ca6dc37",
   "metadata": {},
   "source": [
    "### Restart your Kernel...\n",
    "\n",
    "If you're running this notebook within Jupyter Labs, or the free Amazon SageMaker Studio Lab environment, please restart your kernel to make sure you have the resources to run this code.\n",
    "\n",
    "From the Jupyter menu, select:\n",
    "`Kernel` -> `Restart Kernel...`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "785bb642-2193-4d14-8cec-f848d6ce7913",
   "metadata": {},
   "source": [
    "## Super Simple Simple Chatbot (S3C?)\n",
    "\n",
    "Let's use what we know about prompt engineering to create a simple chatbot.  All the code, including the LLM is running within this notebook environment, so it will be simple, cheap to run, and a good playground for working in. \n",
    "\n",
    "Don't expect great things from this smallish model/chatbot, but have fun! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce603a66-6f2e-4fc2-a224-11f1f2aaaa58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368138b5-a1b6-4e44-99d5-73b848c8d05c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64:' + os.getenv('LD_LIBRARY_PATH', '')\n",
    "os.environ['BITSANDBYTES_NOWELCOME'] = '1'\n",
    "os.environ['CUDA_RUNTIME_LIB'] = '112'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d8fcb-2a39-4d7b-b7ad-73a4f494fca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-xl\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ecd9c66",
   "metadata": {},
   "source": [
    "This first class wraps the calls to the LLM, extracts just the parts of the completion we want, and maintains state in a conversation history.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ad01b-bd72-4a79-bb98-216e34cb2548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self, model_name, init_prompt=[]):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True, max_length=512)\n",
    "        free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "        max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map='auto', load_in_8bit=True, max_memory=max_memory)\n",
    "        self.conversation_history = init_prompt\n",
    "\n",
    "    def generate_response(self, input_text):\n",
    "        self.conversation_history.append(f\"User: {input_text}\")\n",
    "        input_with_history = \"\\n\".join(self.conversation_history)\n",
    "        input_tokens = self.tokenizer.encode(input_with_history, return_tensors='pt').to(self.device)\n",
    "        generated_tokens = self.model.generate(input_tokens, max_length=500, num_return_sequences=1)\n",
    "        response = self.tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        self.conversation_history.append(f\"{response}\")\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76dec539",
   "metadata": {},
   "source": [
    "This next cell contains our starting prompt.  Feel free to experiment with this and see what impact your experiments have on the output of the chatbot.  Note that the prompt is a Python list, and our code will use this as the start of the chat history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c80e166-7ed6-4dd7-aa4c-e18dd8f034ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_prompt = [\n",
    "    \"This is a friendly and safe chat session between a user and a computer called Chatbot.\",\n",
    "    \"Start of chat:\",\n",
    "    \"User: I am a real person with a question to ask. Who are you?\",\n",
    "    \"Chatbot: I am a chatbot, and I am here to help.\",\n",
    "    \"User: Where do you live?\"\n",
    "    \"Chatbot: I am hosted on AWS.\",\n",
    "    \"User: What is a car?\",\n",
    "    \"Chatbot: A car is a four-wheeled road vehicle that is powered by an engine and is able to carry a small number of people.\",\n",
    "    \"User: Do you have a dog?\",\n",
    "    \"Chatbot: As a chatbot, I am incapable of owning a dog.\",\n",
    "    \"User: What color is the Tardis?\",\n",
    "    \"Chatbot: The Tardis is Dr Who's time travel machine, and is blue.\", \n",
    "    \"User: What is the Millennium Falcon?\",\n",
    "    \"Chatbot: The Millennium Falcon is a spaceship from Star Wars.  It has had various captains over the years.\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "354fb98a",
   "metadata": {},
   "source": [
    "Let's load the class and load the chatbot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbfacc96-0b50-461b-8a2a-9713a632675a",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1RjlFQTAiIHN0cm9rZT0iIzVGOUVBMCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1RjlFQTAiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzVGOUVBMCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxOCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIHdoaWxlIHRvIHJ1bi48L3RleHQ+CiAgICA8dGV4dCB4PSIxMDAiIHk9IjU2IiBmb250LWZhbWlseT0iQXJpYWwsIHNhbnMtc2VyaWYiIGZvbnQtc2l6ZT0iMTgiIGZpbGw9IiMzMzMzMzMiPkFuIGFzdGVyaXNrIFsqXSB3aWxsIHNob3cgb24gdGhlIGxlZnQgaGFuZCBzaWRlIG9mIHRoZSBjZWxsIHdoaWxlIGl0J3MgcnVubmluZyw8L3RleHQ+CiAgICA8dGV4dCB4PSIxMDAiIHk9Ijc4IiBmb250LWZhbWlseT0iQXJpYWwsIHNhbnMtc2VyaWYiIGZvbnQtc2l6ZT0iMTgiIGZpbGw9IiMzMzMzMzMiPmFuZCB0aGVuIGJlIHJlcGxhY2VkIGJ5IGEgbnVtYmVyIHdoZW4gaXQncyBkb25lLiBQbGVhc2UgYmUgcGF0aWVudC48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open big\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298ec9b-a5fe-4ec6-8a64-9a2607cafd26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot = ChatBot(model_name, initial_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d21ac98-d5d9-413a-8e5d-98dd3a53280e",
   "metadata": {},
   "source": [
    " <img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc31cebb",
   "metadata": {},
   "source": [
    "Finally, so that we can interact with the chatbot, we use iPyWidgets to render a simple chatbot interface right here in the notebook.  When we run this code, we should see a simple text box and `Send` button.  \n",
    "\n",
    "As a start, ask the chatbot: `What is the capital of Australia?`\n",
    "\n",
    "And our chatbot has 'memory' (in the chat history).  Try something like this: \n",
    "\n",
    "```\n",
    "User: What is the capital of Australia?\n",
    "Chatbot: ...\n",
    "User: Which river runs through there?\n",
    "Chatbot: ...\n",
    "```\n",
    "\n",
    "These are just examples, try anything you like!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559af6bf-5927-4f50-bfe6-bcd2be71be73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def on_send_button_click(button):\n",
    "    input_text = input_field.value\n",
    "    response = chatbot.generate_response(input_text)\n",
    "\n",
    "    with output:\n",
    "        print(\"User:\", input_text)\n",
    "        print(response.strip())\n",
    "\n",
    "    input_field.value = \"\"\n",
    "\n",
    "def on_input_field_submit(text):\n",
    "    on_send_button_click(None)\n",
    "\n",
    "# Create the input field and send button\n",
    "input_field = widgets.Text(placeholder='Type your message here...')\n",
    "send_button = widgets.Button(description='Send')\n",
    "output = widgets.Output()\n",
    "\n",
    "# Assign the function to the button click event and the input field submit event\n",
    "send_button.on_click(on_send_button_click)\n",
    "input_field.on_submit(on_input_field_submit)\n",
    "\n",
    "# Display the chat interface\n",
    "display(output, input_field, send_button)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74cdfa02",
   "metadata": {},
   "source": [
    "## Want to use a different model with this notebook?\n",
    "\n",
    "Hugging Face have many models that you can use and drop in to code like this. But you may need to make  modifications depending on the model you choose. \n",
    "\n",
    "You should be able to drop other T5 based models in by just changing the `model_name` variable.  \n",
    "\n",
    "However decoder only transformer models such as Bloom and the GPT family, use a slightly different library.  Look to replace `AutoModelForSeq2SeqLM` with `AutoModelForCausalLM` both in the `import` statements and the code that loads the models using `.from_pretrained`. \n",
    "\n",
    "For example:\n",
    "\n",
    "- from: `transformers import AutoTokenizer, AutoModelForSeq2SeqLM`\n",
    "- to: `transformers import AutoTokenizer, AutoModelForCausalLM`\n",
    "\n",
    "and \n",
    "\n",
    "- from: `model = AutoModelForSeq2SeqLM.from_pretrained(model_name, ...`\n",
    "- to: `model = AutoModelForCausalLM.from_pretrained(model_name, ...`\n",
    "\n",
    "Keep in mind when you load other models that some LLMs are huge, and you can run out of capacity on the system you are running on very quickly.  Watch out for disk space.  On linux based systems Hugging Face will store a cached version of the model (and tokenizer but typically this is much smaller) to `~/.cache/huggingface`.  When you change model, you might want to delete this cache to make room for the new download.  You can use `rm -rf ~/.cache/huggingface` as needed.  Also when the model loads, it will load into the GPU of the system.  If the model is too big for the GPU you will typically see a kernel crash, and things stop working.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45165022-edfc-4aa5-a56d-8b4ba98710af",
   "metadata": {},
   "source": [
    "### Thanks for working through this notebook!\n",
    "\n",
    "My name is Mike Chambers, and I am a AI/ML Specialist for Amazon Web Services.\n",
    "\n",
    "You can connect with me on [LinkedIn](https://linkedin.com/in/mikegchambers)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05709960",
   "metadata": {},
   "source": [
    "```\n",
    "License Info\n",
    "\n",
    "MIT No Attribution\n",
    "\n",
    "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
